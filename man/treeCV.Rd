% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/treeCV.R
\name{treeCV}
\alias{treeCV}
\title{Performs k-fold cross validation for selection optimal tuning parameter lambda.}
\usage{
treeCV(tre, sp.var, dat, outcome = c("time", "ae"), nfolds = 10,
  param = seq(0, 200, 0.01), AIPWE = FALSE, N0 = 20, n0 = 5,
  sort = TRUE, ctgs = NA, standardize = FALSE,
  stabilize.type = c("linear", "rf"), stabilize = TRUE, scale.ae = FALSE,
  haoda.method = FALSE, haoda.ae.level = NA, reverse.ae.scale = FALSE,
  use.other.nodes = TRUE)
}
\arguments{
\item{tre}{original large tree from grow.ITR()}

\item{sp.var}{specifies the columns of splitting variables in the input data to grow the original tree.}

\item{dat}{data frame used to grow tre}

\item{nfolds}{number of folds in the cross validation. Defaults to 10.}

\item{param}{vector of numeric values to be considered as the tuning parameter. Defaults to seq(0, 0.15, 0.1) but should be modified for each specific problem.}

\item{AIPWE}{logical indicating if the augmented estimator should be used. Defaults to FALSE.}

\item{N0}{sets the minimal number of observations allowed in terminal nodes. Defaults to 20.}

\item{n0}{sets the minimum number of observations from each treatment group to be in each child node.  Defaults to 5.}

\item{sort}{logical indicating if data should be sorted before folds are created. Defaults to FALSE.}

\item{ctgs}{columns of categorical variables.}

\item{stabilize.type}{Type of stabilization. 'rf' for random forests (default) or 'linear' for linear model.}

\item{stabilize}{logical. Should stabilization be used?}

\item{haoda.method}{logical if the conditional tree structure is being used}

\item{haoda.ae.level}{value used if haoda.method is TRUE.}
}
\value{
A summary of the cross validation including optimal penalty parameter and the optimal model.

\item{best.tree}{optimal ITR tree model}

\item{best.lambda}{optimal lambda}

\item{full.tree}{original input tree}

\item{pruned.tree}{pruning output given the optimal lambda}

\item{data}{input data}

\item{details}{summary of model performance for each lambda under consideration}
}
\description{
Performs k-fold cross validation for selection optimal tuning parameter lambda.
}
\examples{

# Grow large tree
set.seed(1)
dat <- gdataM(n=1000, depth = 2, beta1=3, beta2=1)
tre <- grow.ITR(dat, split.var = 1:4)

# This tre should have 3 terminal nodes (correct size), but has 4 as grown.

cv.prune <- treeCV(tre, dat, nfolds = 5, param = seq(0, 0.15, 0.01), sp.var = 1:4)

# The best tree returned has the correct number of nodes
cv.prune$best.tree
#node size n.1 n.0 trt.effect var vname cut.1 cut.2  score
#1    0 1000 479 521  1.2936724   1    X1     r  0.28 0.8768
#2   01  277  86 191 -0.9439306  NA  <NA>  <NA>  <NA>     NA
#3   02  723 393 330  2.0819738   3    X3     r   0.1 0.9426
#5  021   78   9  69 -1.3600766  NA  <NA>  <NA>  <NA>     NA
#4  022  645 384 261  2.4226122  NA  <NA>  <NA>  <NA>     NA


}
